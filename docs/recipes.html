<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Recipes - SQLModel CRUD Utils</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary-color: #3660a5;
            --primary-hover: #1659c5;
            --secondary-color: #488efc;
            --background: #ffffff;
            --text-color: #212529;
            --muted-color: #6c757d;
            --accent-bg: #f8f9fa;
            --border-color: #dee2e6;
            --code-bg: #f8f8f8;
            --success-color: #28a745;
            --info-color: #17a2b8;
            --warning-color: #ffc107;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background);
        }

        header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: 1rem 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
        }

        nav {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
        }

        .logo {
            font-size: 1.5rem;
            font-weight: bold;
            text-decoration: none;
            color: white;
        }

        .nav-links {
            display: flex;
            gap: 2rem;
            list-style: none;
        }

        .nav-links a {
            color: white;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: background-color 0.3s;
        }

        .nav-links a:hover,
        .nav-links a.active {
            background-color: rgba(255, 255, 255, 0.2);
        }

        .page-header {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
            color: white;
            padding: 3rem 2rem;
            text-align: center;
        }

        .page-header h1 {
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
            font-weight: 300;
        }

        .page-header p {
            font-size: 1.1rem;
            opacity: 0.95;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 3rem 2rem;
        }

        .recipe {
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 3rem;
            overflow: hidden;
        }

        .recipe-header {
            background: var(--accent-bg);
            padding: 1.5rem 2rem;
            border-bottom: 3px solid var(--secondary-color);
        }

        .recipe-header h2 {
            color: var(--primary-color);
            font-size: 1.75rem;
            margin-bottom: 0.5rem;
            font-weight: 400;
        }

        .recipe-tags {
            display: flex;
            gap: 0.5rem;
            flex-wrap: wrap;
            margin-top: 0.5rem;
        }

        .tag {
            background: var(--secondary-color);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 12px;
            font-size: 0.85rem;
        }

        .recipe-content {
            padding: 2rem;
        }

        .problem, .code-section, .explanation {
            margin-bottom: 2rem;
        }

        h3 {
            color: var(--primary-color);
            font-size: 1.3rem;
            margin-bottom: 0.75rem;
            font-weight: 400;
        }

        h4 {
            color: var(--text-color);
            font-size: 1.1rem;
            margin: 1.5rem 0 0.75rem;
        }

        .code-example {
            background: var(--code-bg);
            border-radius: 8px;
            overflow: hidden;
            margin: 1rem 0;
        }

        .code-header {
            background: var(--primary-color);
            color: white;
            padding: 0.75rem 1rem;
            font-size: 0.875rem;
            font-weight: 500;
        }

        pre {
            margin: 0;
            padding: 1rem;
            overflow-x: auto;
        }

        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
        }

        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }

        li {
            margin-bottom: 0.5rem;
        }

        .tip {
            background: #e7f3ff;
            border-left: 4px solid var(--info-color);
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }

        .warning {
            background: #fff3cd;
            border-left: 4px solid var(--warning-color);
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 4px;
        }

        .key-points {
            background: var(--accent-bg);
            padding: 1.5rem;
            border-radius: 8px;
            margin: 1rem 0;
        }

        .key-points h4 {
            margin-top: 0;
        }

        .key-points ul {
            margin-bottom: 0;
        }

        footer {
            background-color: #2c3e50;
            color: white;
            padding: 2rem 0;
            text-align: center;
        }

        footer a {
            color: var(--secondary-color);
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .page-header h1 {
                font-size: 2rem;
            }

            .nav-links {
                flex-direction: column;
                gap: 0.5rem;
                width: 100%;
                margin-top: 1rem;
            }
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <a href="index.html" class="logo">SQLModel CRUD Utils</a>
            <ul class="nav-links">
                <li><a href="index.html">Home</a></li>
                <li><a href="use-cases.html">Use Cases</a></li>
                <li><a href="recipes.html" class="active">Recipes</a></li>
                <li><a href="sqlmodel_crud_utils.html">API Reference</a></li>
                <li><a href="https://github.com/fsecada01/SQLModel-CRUD-Utilities">GitHub</a></li>
            </ul>
        </nav>
    </header>

    <div class="page-header">
        <h1>Practical Recipes</h1>
        <p>Copy-paste ready code patterns for common database operations</p>
    </div>

    <div class="container">
        <!-- Recipe 1: Complex Filtering -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Complex Filtering and Search</h2>
                <p>Implement advanced search with multiple filter conditions</p>
                <div class="recipe-tags">
                    <span class="tag">Filtering</span>
                    <span class="tag">Search</span>
                    <span class="tag">Querying</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>You need to implement a search feature that supports multiple filters, date ranges, text search, and sorting.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Advanced product search with multiple filters</div>
                        <pre><code class="language-python">from sqlmodel import Session, Field, SQLModel
from sqlmodel_crud_utils import get_rows
from typing import Optional
from datetime import datetime

class Product(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str = Field(index=True)
    description: str
    price: float
    category: str = Field(index=True)
    stock: int
    created_at: datetime = Field(default_factory=datetime.utcnow, index=True)
    is_active: bool = Field(default=True)

def search_products(
    session: Session,
    search_term: Optional[str] = None,
    category: Optional[str] = None,
    min_price: Optional[float] = None,
    max_price: Optional[float] = None,
    in_stock_only: bool = False,
    created_after: Optional[datetime] = None,
    created_before: Optional[datetime] = None,
    page: int = 1,
    page_size: int = 20,
    sort_by: str = "created_at",
    sort_desc: bool = True
):
    """
    Advanced product search with multiple filter options
    """
    # Build filters dynamically
    filters = {"is_active": True}

    # Text search (partial match)
    if search_term:
        filters["name__like"] = f"%{search_term}%"

    # Category filter (exact match)
    if category:
        filters["category"] = category

    # Price range filters
    if min_price is not None:
        filters["price__gte"] = min_price
    if max_price is not None:
        filters["price__lte"] = max_price

    # Stock filter
    if in_stock_only:
        filters["stock__gt"] = 0

    # Date range filters
    if created_after:
        filters["created_at__gte"] = created_after
    if created_before:
        filters["created_at__lte"] = created_before

    # Calculate pagination
    offset = (page - 1) * page_size

    # Execute query
    success, products = get_rows(
        session_inst=session,
        model=Product,
        offset=offset,
        limit=page_size,
        sort_field=sort_by,
        sort_desc=sort_desc,
        **filters
    )

    return {
        "products": products if success else [],
        "page": page,
        "page_size": page_size,
        "total": len(products) if success else 0
    }

# Usage examples
with Session(engine) as session:
    # Search for electronics under $100
    results = search_products(
        session,
        category="electronics",
        max_price=100,
        in_stock_only=True
    )

    # Search for products created this month
    from datetime import datetime, timedelta
    thirty_days_ago = datetime.utcnow() - timedelta(days=30)
    recent = search_products(
        session,
        created_after=thirty_days_ago,
        sort_by="created_at",
        sort_desc=True
    )

    # Text search with pagination
    page_2 = search_products(
        session,
        search_term="laptop",
        page=2,
        page_size=10
    )</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Explanation</h3>
                    <p>This recipe demonstrates how to:</p>
                    <ul>
                        <li>Use comparison operators (<code>__like</code>, <code>__gte</code>, <code>__lte</code>, <code>__gt</code>) for flexible filtering</li>
                        <li>Build filters dynamically based on user input</li>
                        <li>Combine multiple filter conditions</li>
                        <li>Implement pagination with calculated offsets</li>
                        <li>Support custom sorting</li>
                    </ul>

                    <div class="key-points">
                        <h4>Key Points</h4>
                        <ul>
                            <li>Filters are combined with AND logic by default</li>
                            <li>Use <code>__like</code> for partial text matching with wildcards</li>
                            <li>Index frequently filtered fields for better performance</li>
                            <li>Always validate user inputs before building filters</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Recipe 2: Cursor-Based Pagination -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Cursor-Based Pagination</h2>
                <p>Implement efficient pagination for real-time data</p>
                <div class="recipe-tags">
                    <span class="tag">Pagination</span>
                    <span class="tag">Performance</span>
                    <span class="tag">Scalability</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>Offset-based pagination becomes slow with large datasets and can miss records when data is inserted during pagination. You need cursor-based pagination for better performance and consistency.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Cursor-based pagination implementation</div>
                        <pre><code class="language-python">from sqlmodel import Session, Field, SQLModel
from sqlmodel_crud_utils import get_rows
from typing import Optional, List
from datetime import datetime

class Post(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    title: str
    content: str
    created_at: datetime = Field(default_factory=datetime.utcnow, index=True)
    is_published: bool = Field(default=True)

def get_posts_cursor_paginated(
    session: Session,
    cursor: Optional[int] = None,
    limit: int = 20,
    is_published: bool = True
) -> dict:
    """
    Get posts using cursor-based pagination
    Cursor is the ID of the last post from the previous page
    """
    filters = {"is_published": is_published}

    # If cursor provided, get posts with ID greater than cursor
    if cursor:
        filters["id__gt"] = cursor

    success, posts = get_rows(
        session_inst=session,
        model=Post,
        limit=limit,
        sort_field="id",
        sort_desc=False,  # Important: consistent sort order
        **filters
    )

    posts = posts if success else []

    # Generate next cursor
    next_cursor = posts[-1].id if posts else None

    return {
        "posts": posts,
        "next_cursor": next_cursor,
        "has_more": len(posts) == limit
    }

def get_posts_time_based_cursor(
    session: Session,
    cursor_time: Optional[datetime] = None,
    cursor_id: Optional[int] = None,
    limit: int = 20
) -> dict:
    """
    Time-based cursor pagination (for chronological feeds)
    Uses created_at + id for stable pagination
    """
    filters = {"is_published": True}

    if cursor_time and cursor_id:
        # Get posts created before cursor_time
        # OR posts created at cursor_time but with id > cursor_id
        # This requires custom query - here's a simplified version
        filters["created_at__lte"] = cursor_time
        filters["id__gt"] = cursor_id

    success, posts = get_rows(
        session_inst=session,
        model=Post,
        limit=limit,
        sort_field="created_at",
        sort_desc=True,
        **filters
    )

    posts = posts if success else []

    # Generate next cursor
    if posts:
        last_post = posts[-1]
        next_cursor = {
            "time": last_post.created_at.isoformat(),
            "id": last_post.id
        }
    else:
        next_cursor = None

    return {
        "posts": posts,
        "next_cursor": next_cursor,
        "has_more": len(posts) == limit
    }

# Usage in FastAPI
from fastapi import APIRouter, Query

router = APIRouter()

@router.get("/posts")
async def list_posts(
    cursor: Optional[int] = Query(None),
    limit: int = Query(20, le=100),
    session: Session = Depends(get_session)
):
    """Get posts with cursor pagination"""
    return get_posts_cursor_paginated(session, cursor, limit)

# Client usage example
def fetch_all_posts(session: Session):
    """Fetch all posts using cursor pagination"""
    all_posts = []
    cursor = None

    while True:
        result = get_posts_cursor_paginated(session, cursor=cursor, limit=100)
        all_posts.extend(result["posts"])

        if not result["has_more"]:
            break

        cursor = result["next_cursor"]

    return all_posts</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Explanation</h3>
                    <p>Cursor-based pagination uses a unique identifier (like ID or timestamp) as a marker instead of offset:</p>
                    <ul>
                        <li><strong>ID-based cursor:</strong> Simple and efficient for most cases</li>
                        <li><strong>Time-based cursor:</strong> Better for chronological feeds</li>
                        <li><strong>Consistent ordering:</strong> Critical for stable pagination</li>
                        <li><strong>No offset arithmetic:</strong> Better performance with large datasets</li>
                    </ul>

                    <div class="tip">
                        <strong>Pro Tip:</strong> For reverse chronological feeds (newest first), use <code>id__lt</code> instead of <code>id__gt</code> and sort descending.
                    </div>
                </div>
            </div>
        </div>

        <!-- Recipe 3: Batch Processing with Error Handling -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Batch Processing with Error Handling</h2>
                <p>Process large datasets efficiently with graceful error handling</p>
                <div class="recipe-tags">
                    <span class="tag">Batch Operations</span>
                    <span class="tag">Error Handling</span>
                    <span class="tag">Performance</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>You need to process thousands of records, but don't want a single failure to stop the entire batch. You need proper error tracking and the ability to retry failed records.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Batch processing with error recovery</div>
                        <pre><code class="language-python">from sqlmodel import Session, Field, SQLModel
from sqlmodel_crud_utils import (
    insert_data_rows, get_rows, update_row,
    transaction, BulkOperationError
)
from typing import List, Dict, Any, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImportRecord(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    external_id: str = Field(index=True, unique=True)
    data: str
    status: str = Field(default="pending")  # pending, processing, success, failed
    error_message: Optional[str] = None
    retry_count: int = Field(default=0)

def process_batch(
    session: Session,
    records: List[ImportRecord],
    batch_size: int = 100
) -> Dict[str, Any]:
    """
    Process records in batches with error handling
    """
    total_success = 0
    total_failed = 0
    failed_records = []

    # Process in batches
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        logger.info(f"Processing batch {i // batch_size + 1}: {len(batch)} records")

        try:
            with transaction(session) as tx:
                # Try bulk insert first
                success = insert_data_rows(batch, tx)
                if success:
                    total_success += len(batch)
                    logger.info(f"Batch inserted successfully: {len(batch)} records")
                else:
                    # Bulk insert failed, try individual inserts
                    logger.warning("Bulk insert failed, trying individual inserts")
                    for record in batch:
                        try:
                            write_row(record, tx)
                            total_success += 1
                        except Exception as e:
                            total_failed += 1
                            failed_records.append({
                                "record": record,
                                "error": str(e)
                            })
                            logger.error(f"Failed to insert record {record.external_id}: {e}")
        except BulkOperationError as e:
            logger.error(f"Batch operation error: {e}")
            total_failed += len(batch)
            failed_records.extend([
                {"record": r, "error": str(e)}
                for r in batch
            ])

    return {
        "total_processed": len(records),
        "total_success": total_success,
        "total_failed": total_failed,
        "failed_records": failed_records
    }

def process_with_retry(
    session: Session,
    max_retries: int = 3
) -> Dict[str, Any]:
    """
    Process pending records with retry logic
    """
    # Get pending records
    success, pending = get_rows(
        session_inst=session,
        model=ImportRecord,
        status="pending",
        retry_count__lt=max_retries,
        limit=1000
    )

    if not success or not pending:
        return {"message": "No pending records"}

    results = {"processed": 0, "succeeded": 0, "failed": 0, "max_retries_exceeded": 0}

    for record in pending:
        # Mark as processing
        update_row(
            id_str=record.id,
            data_dict={"status": "processing"},
            model=ImportRecord,
            session_inst=session
        )

        try:
            # Simulate processing
            # In real scenario, this would be your business logic
            process_record(record)

            # Mark as success
            update_row(
                id_str=record.id,
                data_dict={"status": "success", "error_message": None},
                model=ImportRecord,
                session_inst=session
            )
            results["succeeded"] += 1

        except Exception as e:
            logger.error(f"Error processing record {record.id}: {e}")

            # Increment retry count
            new_retry_count = record.retry_count + 1

            if new_retry_count >= max_retries:
                # Max retries exceeded
                update_row(
                    id_str=record.id,
                    data_dict={
                        "status": "failed",
                        "retry_count": new_retry_count,
                        "error_message": f"Max retries exceeded: {str(e)}"
                    },
                    model=ImportRecord,
                    session_inst=session
                )
                results["max_retries_exceeded"] += 1
            else:
                # Mark for retry
                update_row(
                    id_str=record.id,
                    data_dict={
                        "status": "pending",
                        "retry_count": new_retry_count,
                        "error_message": str(e)
                    },
                    model=ImportRecord,
                    session_inst=session
                )
                results["failed"] += 1

        results["processed"] += 1

    return results

def process_record(record: ImportRecord):
    """
    Simulate record processing
    Replace with actual business logic
    """
    # Your processing logic here
    import json
    data = json.loads(record.data)
    # ... process data ...
    pass

# Usage
with Session(engine) as session:
    # Create sample records
    records = [
        ImportRecord(external_id=f"ext_{i}", data='{"value": 123}')
        for i in range(1000)
    ]

    # Process in batches
    results = process_batch(session, records, batch_size=100)
    print(f"Success: {results['total_success']}, Failed: {results['total_failed']}")

    # Process with retry logic
    retry_results = process_with_retry(session, max_retries=3)
    print(f"Retry results: {retry_results}")</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Explanation</h3>
                    <p>This recipe shows how to:</p>
                    <ul>
                        <li>Process large datasets in manageable batches</li>
                        <li>Use bulk operations with fallback to individual processing</li>
                        <li>Track processing status in the database</li>
                        <li>Implement retry logic with exponential backoff</li>
                        <li>Log errors without stopping the entire process</li>
                        <li>Report detailed results with success/failure counts</li>
                    </ul>

                    <div class="warning">
                        <strong>Warning:</strong> Always use transactions for batch operations to ensure data consistency. If a batch fails, all changes in that batch are rolled back.
                    </div>
                </div>
            </div>
        </div>

        <!-- Recipe 4: Optimistic Locking -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Optimistic Locking for Concurrent Updates</h2>
                <p>Prevent lost updates in multi-user environments</p>
                <div class="recipe-tags">
                    <span class="tag">Concurrency</span>
                    <span class="tag">Race Conditions</span>
                    <span class="tag">Data Integrity</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>Multiple users can update the same record simultaneously, leading to lost updates. You need to detect and handle concurrent modifications.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Optimistic locking with version field</div>
                        <pre><code class="language-python">from sqlmodel import Session, Field, SQLModel
from sqlmodel_crud_utils import get_row, update_row, AuditMixin
from typing import Optional
from datetime import datetime

class BankAccount(SQLModel, AuditMixin, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    account_number: str = Field(index=True, unique=True)
    balance: float
    version: int = Field(default=0)  # Optimistic locking version

class ConcurrentUpdateError(Exception):
    """Raised when concurrent update is detected"""
    pass

def update_balance_safe(
    session: Session,
    account_id: int,
    amount: float,
    expected_version: int,
    user: str
) -> BankAccount:
    """
    Update account balance with optimistic locking
    """
    # Get current account state
    success, account = get_row(
        id_str=account_id,
        session_inst=session,
        model=BankAccount
    )

    if not success:
        raise ValueError(f"Account {account_id} not found")

    # Check version matches (no concurrent updates)
    if account.version != expected_version:
        raise ConcurrentUpdateError(
            f"Account was modified by another user. "
            f"Expected version {expected_version}, got {account.version}"
        )

    # Calculate new balance
    new_balance = account.balance + amount
    if new_balance < 0:
        raise ValueError("Insufficient funds")

    # Update with version increment
    success, updated = update_row(
        id_str=account_id,
        data_dict={
            "balance": new_balance,
            "version": account.version + 1,
            "updated_by": user
        },
        model=BankAccount,
        session_inst=session
    )

    if not success:
        raise RuntimeError("Update failed")

    return updated

def transfer_with_optimistic_locking(
    session: Session,
    from_account_id: int,
    to_account_id: int,
    amount: float,
    user: str,
    max_retries: int = 3
) -> dict:
    """
    Transfer money between accounts with retry on concurrent updates
    """
    for attempt in range(max_retries):
        try:
            # Get current versions
            _, from_account = get_row(
                id_str=from_account_id,
                session_inst=session,
                model=BankAccount
            )
            _, to_account = get_row(
                id_str=to_account_id,
                session_inst=session,
                model=BankAccount
            )

            from_version = from_account.version
            to_version = to_account.version

            # Perform transfer
            with transaction(session) as tx:
                # Debit from source
                update_balance_safe(
                    tx, from_account_id, -amount, from_version, user
                )
                # Credit to destination
                update_balance_safe(
                    tx, to_account_id, amount, to_version, user
                )

            return {
                "success": True,
                "attempts": attempt + 1,
                "message": f"Transferred {amount} successfully"
            }

        except ConcurrentUpdateError as e:
            if attempt == max_retries - 1:
                return {
                    "success": False,
                    "attempts": attempt + 1,
                    "error": f"Max retries exceeded: {str(e)}"
                }
            # Wait briefly before retry (exponential backoff)
            import time
            time.sleep(0.1 * (2 ** attempt))
            continue

    return {"success": False, "error": "Unexpected error"}

# Usage example
with Session(engine) as session:
    # Create accounts
    acc1 = BankAccount(account_number="ACC001", balance=1000.0, created_by="system")
    acc2 = BankAccount(account_number="ACC002", balance=500.0, created_by="system")

    write_row(acc1, session)
    write_row(acc2, session)

    # Transfer with optimistic locking
    result = transfer_with_optimistic_locking(
        session,
        from_account_id=acc1.id,
        to_account_id=acc2.id,
        amount=200.0,
        user="admin"
    )

    print(f"Transfer result: {result}")

# FastAPI endpoint with optimistic locking
from fastapi import HTTPException

@app.post("/accounts/{account_id}/deposit")
async def deposit(
    account_id: int,
    amount: float,
    version: int,
    session: Session = Depends(get_session)
):
    """Deposit money with optimistic locking"""
    try:
        updated_account = update_balance_safe(
            session,
            account_id,
            amount,
            expected_version=version,
            user="api"
        )
        return {
            "balance": updated_account.balance,
            "version": updated_account.version
        }
    except ConcurrentUpdateError as e:
        raise HTTPException(status_code=409, detail=str(e))
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Explanation</h3>
                    <p>Optimistic locking prevents lost updates by:</p>
                    <ul>
                        <li>Adding a <code>version</code> field that increments on each update</li>
                        <li>Checking the version before updating</li>
                        <li>Raising an error if the version changed (concurrent update detected)</li>
                        <li>Implementing retry logic with exponential backoff</li>
                    </ul>

                    <div class="key-points">
                        <h4>When to Use Optimistic Locking</h4>
                        <ul>
                            <li>Financial transactions (accounts, payments)</li>
                            <li>Inventory management</li>
                            <li>Document editing with multiple users</li>
                            <li>Any scenario where concurrent updates could cause data loss</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Recipe 5: Cascading Soft Deletes -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Cascading Soft Deletes</h2>
                <p>Safely delete parent records and all related children</p>
                <div class="recipe-tags">
                    <span class="tag">Soft Delete</span>
                    <span class="tag">Relationships</span>
                    <span class="tag">Data Integrity</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>When soft-deleting a parent record, you need to also soft-delete all related child records to maintain referential integrity and prevent orphaned records.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Cascading soft deletes with relationships</div>
                        <pre><code class="language-python">from sqlmodel import Session, Field, SQLModel, Relationship
from sqlmodel_crud_utils import SoftDeleteMixin, get_rows, transaction
from typing import Optional, List

class Organization(SQLModel, SoftDeleteMixin, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    # Relationship to teams
    teams: List["Team"] = Relationship(back_populates="organization")

class Team(SQLModel, SoftDeleteMixin, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    organization_id: int = Field(foreign_key="organization.id")
    # Relationships
    organization: Organization = Relationship(back_populates="teams")
    members: List["TeamMember"] = Relationship(back_populates="team")

class TeamMember(SQLModel, SoftDeleteMixin, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    email: str
    team_id: int = Field(foreign_key="team.id")
    # Relationship
    team: Team = Relationship(back_populates="members")

def soft_delete_organization_cascade(
    session: Session,
    org_id: int,
    deleted_by: str
) -> dict:
    """
    Soft delete an organization and all its teams and members
    """
    # Get organization with relationships
    success, org = get_row(
        id_str=org_id,
        session_inst=session,
        model=Organization,
        selectinload_list=["teams"]
    )

    if not success or org.is_deleted:
        return {"success": False, "error": "Organization not found"}

    stats = {
        "organizations": 0,
        "teams": 0,
        "members": 0
    }

    with transaction(session) as tx:
        # Get all teams for this organization
        _, teams = get_rows(
            session_inst=tx,
            model=Team,
            organization_id=org_id,
            is_deleted=False
        )

        # Soft delete all members in all teams
        for team in teams:
            _, members = get_rows(
                session_inst=tx,
                model=TeamMember,
                team_id=team.id,
                is_deleted=False
            )

            for member in members:
                member.soft_delete(user=deleted_by)
                tx.add(member)
                stats["members"] += 1

            # Soft delete the team
            team.soft_delete(user=deleted_by)
            tx.add(team)
            stats["teams"] += 1

        # Soft delete the organization
        org.soft_delete(user=deleted_by)
        tx.add(org)
        stats["organizations"] += 1

    return {
        "success": True,
        "deleted": stats,
        "message": f"Deleted organization and {stats['teams']} teams with {stats['members']} members"
    }

def restore_organization_cascade(
    session: Session,
    org_id: int
) -> dict:
    """
    Restore a soft-deleted organization and all its teams and members
    """
    # Get organization (including soft-deleted)
    success, org = get_row(
        id_str=org_id,
        session_inst=session,
        model=Organization
    )

    if not success or not org.is_deleted:
        return {"success": False, "error": "Organization not found or not deleted"}

    stats = {"organizations": 0, "teams": 0, "members": 0}

    with transaction(session) as tx:
        # Restore organization
        org.restore()
        tx.add(org)
        stats["organizations"] += 1

        # Get all soft-deleted teams for this organization
        _, teams = get_rows(
            session_inst=tx,
            model=Team,
            organization_id=org_id,
            is_deleted=True
        )

        for team in teams:
            # Restore team
            team.restore()
            tx.add(team)
            stats["teams"] += 1

            # Get all soft-deleted members for this team
            _, members = get_rows(
                session_inst=tx,
                model=TeamMember,
                team_id=team.id,
                is_deleted=True
            )

            for member in members:
                member.restore()
                tx.add(member)
                stats["members"] += 1

    return {
        "success": True,
        "restored": stats,
        "message": f"Restored organization with {stats['teams']} teams and {stats['members']} members"
    }

# Usage
with Session(engine) as session:
    # Create organization structure
    org = Organization(name="Acme Corp")
    write_row(org, session)

    team = Team(name="Engineering", organization_id=org.id)
    write_row(team, session)

    member = TeamMember(name="John Doe", email="john@acme.com", team_id=team.id)
    write_row(member, session)

    # Cascading soft delete
    result = soft_delete_organization_cascade(session, org.id, deleted_by="admin")
    print(result)
    # Output: {'success': True, 'deleted': {'organizations': 1, 'teams': 1, 'members': 1}, ...}

    # Restore everything
    restore_result = restore_organization_cascade(session, org.id)
    print(restore_result)</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Explanation</h3>
                    <p>This recipe demonstrates:</p>
                    <ul>
                        <li>Recursively soft-deleting related records</li>
                        <li>Maintaining referential integrity with soft deletes</li>
                        <li>Using transactions to ensure all-or-nothing deletion</li>
                        <li>Tracking deletion statistics</li>
                        <li>Implementing restore functionality</li>
                    </ul>

                    <div class="tip">
                        <strong>Pro Tip:</strong> Consider adding a <code>deleted_cascade_id</code> field to track which parent deletion caused a child to be deleted. This makes it easier to restore related records together.
                    </div>
                </div>
            </div>
        </div>

        <!-- Recipe 6: Performance Optimization -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Performance Optimization Tips</h2>
                <p>Make your database queries faster and more efficient</p>
                <div class="recipe-tags">
                    <span class="tag">Performance</span>
                    <span class="tag">Optimization</span>
                    <span class="tag">Best Practices</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>Your application is slow when querying large datasets or dealing with complex relationships. You need to optimize database operations without major refactoring.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Performance optimization techniques</div>
                        <pre><code class="language-python">from sqlmodel import Session, Field, SQLModel, Relationship
from sqlmodel_crud_utils import get_rows, get_row
from typing import Optional, List

# 1. Use indexes on frequently queried fields
class User(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    email: str = Field(index=True, unique=True)  # Index for lookups
    username: str = Field(index=True)  # Index for searches
    created_at: datetime = Field(index=True)  # Index for date filtering
    status: str = Field(index=True)  # Index for status filtering
    posts: List["Post"] = Relationship(back_populates="author")

class Post(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    title: str = Field(index=True)
    author_id: int = Field(foreign_key="user.id", index=True)  # Foreign key index
    created_at: datetime = Field(index=True)
    author: User = Relationship(back_populates="posts")

# 2. Eager load relationships to avoid N+1 queries
def get_user_with_posts_efficient(session: Session, user_id: int):
    """Load user and all posts in single query"""
    success, user = get_row(
        id_str=user_id,
        session_inst=session,
        model=User,
        selectinload_list=["posts"]  # Eager load posts
    )
    return user

# BAD: N+1 query problem
def get_users_with_post_count_slow(session: Session):
    """This creates N+1 queries - AVOID THIS"""
    _, users = get_rows(session_inst=session, model=User, limit=100)
    result = []
    for user in users:
        # Each iteration queries the database again!
        _, posts = get_rows(
            session_inst=session,
            model=Post,
            author_id=user.id
        )
        result.append({
            "user": user,
            "post_count": len(posts)
        })
    return result

# GOOD: Single query with relationship loading
def get_users_with_post_count_fast(session: Session):
    """Load all data in one query"""
    _, users = get_rows(
        session_inst=session,
        model=User,
        selectinload_list=["posts"],  # Load all posts for all users
        limit=100
    )
    return [
        {
            "user": user,
            "post_count": len(user.posts)
        }
        for user in users
    ]

# 3. Limit the fields you select (if possible with custom queries)
# Note: sqlmodel_crud_utils loads full models, but you can optimize
# by only loading necessary relationships

# 4. Use pagination for large datasets
def get_all_users_paginated(session: Session, batch_size: int = 100):
    """Process large datasets in batches"""
    offset = 0
    while True:
        _, batch = get_rows(
            session_inst=session,
            model=User,
            offset=offset,
            limit=batch_size,
            sort_field="id"
        )

        if not batch:
            break

        yield batch
        offset += batch_size

# Usage
for user_batch in get_all_users_paginated(session):
    process_users(user_batch)

# 5. Batch operations for inserts/updates
def bulk_update_efficient(session: Session, user_ids: List[int], updates: dict):
    """More efficient than individual updates"""
    from sqlmodel_crud_utils import bulk_upsert_mappings

    mappings = [
        {"id": user_id, **updates}
        for user_id in user_ids
    ]

    bulk_upsert_mappings(
        session_inst=session,
        model=User,
        data_mappings=mappings
    )

# 6. Use connection pooling for concurrent access
from sqlmodel import create_engine

# Configure connection pool
engine = create_engine(
    "postgresql://localhost/mydb",
    pool_size=10,  # Number of persistent connections
    max_overflow=20,  # Additional connections if needed
    pool_pre_ping=True,  # Verify connections before use
    pool_recycle=3600  # Recycle connections after 1 hour
)

# 7. Add composite indexes for common filter combinations
# In your Alembic migration:
"""
def upgrade():
    op.create_index(
        'ix_user_status_created',
        'user',
        ['status', 'created_at']
    )
"""

# Then query efficiently:
def get_active_users_recent(session: Session):
    """Uses composite index on (status, created_at)"""
    from datetime import datetime, timedelta
    thirty_days_ago = datetime.utcnow() - timedelta(days=30)

    _, users = get_rows(
        session_inst=session,
        model=User,
        status="active",
        created_at__gte=thirty_days_ago,
        sort_field="created_at",
        limit=1000
    )
    return users</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Key Optimization Techniques</h3>
                    <ul>
                        <li><strong>Indexes:</strong> Add indexes to fields used in WHERE, ORDER BY, and JOIN clauses</li>
                        <li><strong>Eager Loading:</strong> Use <code>selectinload_list</code> to prevent N+1 queries</li>
                        <li><strong>Pagination:</strong> Always limit query results, use pagination for large datasets</li>
                        <li><strong>Batch Operations:</strong> Use bulk insert/update instead of individual operations</li>
                        <li><strong>Connection Pooling:</strong> Reuse database connections efficiently</li>
                        <li><strong>Composite Indexes:</strong> Create indexes for common filter combinations</li>
                    </ul>

                    <div class="key-points">
                        <h4>Performance Checklist</h4>
                        <ul>
                            <li>✓ Index all foreign keys</li>
                            <li>✓ Index fields used in filters and sorting</li>
                            <li>✓ Use eager loading for relationships</li>
                            <li>✓ Implement pagination (limit + offset or cursor)</li>
                            <li>✓ Use bulk operations for multiple records</li>
                            <li>✓ Configure appropriate connection pool size</li>
                            <li>✓ Monitor slow queries and add indexes</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Recipe 7: Testing Patterns -->
        <div class="recipe">
            <div class="recipe-header">
                <h2>Testing CRUD Operations</h2>
                <p>Write effective tests for database operations</p>
                <div class="recipe-tags">
                    <span class="tag">Testing</span>
                    <span class="tag">Pytest</span>
                    <span class="tag">Best Practices</span>
                </div>
            </div>
            <div class="recipe-content">
                <div class="problem">
                    <h3>Problem</h3>
                    <p>You need to test CRUD operations thoroughly without polluting your production database, with proper fixtures and cleanup.</p>
                </div>

                <div class="code-section">
                    <h3>Solution</h3>
                    <div class="code-example">
                        <div class="code-header">Test fixtures and patterns with pytest</div>
                        <pre><code class="language-python"># tests/conftest.py
import pytest
from sqlmodel import Session, create_engine, SQLModel
from sqlalchemy.pool import StaticPool

@pytest.fixture(name="engine")
def engine_fixture():
    """Create in-memory SQLite database for testing"""
    engine = create_engine(
        "sqlite:///:memory:",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )
    SQLModel.metadata.create_all(engine)
    yield engine
    SQLModel.metadata.drop_all(engine)

@pytest.fixture(name="session")
def session_fixture(engine):
    """Create database session for tests"""
    with Session(engine) as session:
        yield session
        session.rollback()  # Clean up after each test

# tests/test_user_crud.py
from sqlmodel_crud_utils import (
    get_row, get_rows, write_row, update_row, delete_row,
    RecordNotFoundError
)
from models import User

def test_create_user(session):
    """Test creating a user"""
    user = User(
        username="testuser",
        email="test@example.com",
        created_by="test"
    )

    created_user = write_row(user, session)

    assert created_user.id is not None
    assert created_user.username == "testuser"
    assert created_user.email == "test@example.com"

def test_get_user_by_id(session):
    """Test retrieving a user by ID"""
    # Arrange
    user = write_row(
        User(username="testuser", email="test@example.com"),
        session
    )

    # Act
    success, retrieved_user = get_row(
        id_str=user.id,
        session_inst=session,
        model=User
    )

    # Assert
    assert success is True
    assert retrieved_user.id == user.id
    assert retrieved_user.username == "testuser"

def test_get_nonexistent_user(session):
    """Test retrieving a non-existent user"""
    success, user = get_row(
        id_str=999,
        session_inst=session,
        model=User
    )

    assert success is False
    assert user is None

def test_filter_users(session):
    """Test filtering users"""
    # Arrange - create multiple users
    users = [
        User(username=f"user{i}", email=f"user{i}@example.com", is_active=i % 2 == 0)
        for i in range(10)
    ]
    for user in users:
        write_row(user, session)

    # Act - filter active users
    success, active_users = get_rows(
        session_inst=session,
        model=User,
        is_active=True
    )

    # Assert
    assert success is True
    assert len(active_users) == 5
    assert all(u.is_active for u in active_users)

def test_update_user(session):
    """Test updating a user"""
    # Arrange
    user = write_row(
        User(username="oldname", email="old@example.com"),
        session
    )

    # Act
    success, updated_user = update_row(
        id_str=user.id,
        data_dict={"username": "newname", "email": "new@example.com"},
        model=User,
        session_inst=session
    )

    # Assert
    assert success is True
    assert updated_user.username == "newname"
    assert updated_user.email == "new@example.com"

def test_soft_delete_user(session):
    """Test soft deleting a user"""
    # Arrange
    user = write_row(
        User(username="testuser", email="test@example.com"),
        session
    )

    # Act
    user.soft_delete(user="test")
    session.add(user)
    session.commit()
    session.refresh(user)

    # Assert
    assert user.is_deleted is True
    assert user.deleted_at is not None
    assert user.deleted_by == "test"

def test_pagination(session):
    """Test pagination"""
    # Arrange - create 50 users
    for i in range(50):
        write_row(
            User(username=f"user{i}", email=f"user{i}@example.com"),
            session
        )

    # Act - get page 2 (users 10-19)
    success, page2_users = get_rows(
        session_inst=session,
        model=User,
        offset=10,
        limit=10,
        sort_field="id"
    )

    # Assert
    assert success is True
    assert len(page2_users) == 10

# Test factories for complex scenarios
class UserFactory:
    """Factory for creating test users"""

    @staticmethod
    def create_user(session, **kwargs):
        defaults = {
            "username": "testuser",
            "email": "test@example.com",
            "is_active": True,
            "created_by": "test"
        }
        defaults.update(kwargs)
        user = User(**defaults)
        return write_row(user, session)

    @staticmethod
    def create_batch(session, count=5, **kwargs):
        return [
            UserFactory.create_user(
                session,
                username=f"user{i}",
                email=f"user{i}@example.com",
                **kwargs
            )
            for i in range(count)
        ]

def test_with_factory(session):
    """Test using factory"""
    users = UserFactory.create_batch(session, count=10, is_active=True)

    assert len(users) == 10
    assert all(u.is_active for u in users)</code></pre>
                    </div>
                </div>

                <div class="explanation">
                    <h3>Testing Best Practices</h3>
                    <ul>
                        <li>Use in-memory SQLite for fast, isolated tests</li>
                        <li>Create fixtures for database setup and teardown</li>
                        <li>Use factories to create test data easily</li>
                        <li>Follow Arrange-Act-Assert pattern</li>
                        <li>Test both success and failure cases</li>
                        <li>Use transactions to rollback after each test</li>
                    </ul>

                    <div class="tip">
                        <strong>Pro Tip:</strong> Run tests with <code>pytest -v --cov=sqlmodel_crud_utils</code> to see test coverage and identify untested code paths.
                    </div>
                </div>
            </div>
        </div>

    </div>

    <footer>
        <div class="container">
            <p>&copy; 2024 SQLModel CRUD Utils. Licensed under the <a href="https://github.com/fsecada01/SQLModel-CRUD-Utilities/blob/main/LICENSE">MIT License</a>.</p>
            <p>
                <a href="https://github.com/fsecada01/SQLModel-CRUD-Utilities">GitHub</a> |
                <a href="https://pypi.org/project/sqlmodel-crud-utils/">PyPI</a> |
                <a href="https://github.com/fsecada01/SQLModel-CRUD-Utilities/issues">Issues</a>
            </p>
        </div>
    </footer>

    <script>
        hljs.highlightAll();
    </script>
</body>
</html>
